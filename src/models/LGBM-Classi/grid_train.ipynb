{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "2a008d38f9fcd3b1bcdf3424c49b99420c19e99d92e50a69ba9e08b63c274acd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../../data/feature/cbasi_train.csv')\n",
    "val_df = pd.read_csv('../../../data/feature/cbasi_validation.csv')\n",
    "X_tr = train_df.iloc[:, :-1]\n",
    "y_tr = train_df.iloc[:,-1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['boosting_type', 'class_weight', 'colsample_bytree', 'importance_type', 'learning_rate', 'max_depth', 'min_child_samples', 'min_child_weight', 'min_split_gain', 'n_estimators', 'n_jobs', 'num_leaves', 'objective', 'random_state', 'reg_alpha', 'reg_lambda', 'silent', 'subsample', 'subsample_for_bin', 'subsample_freq', 'num_class', 'max_bin'])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "params = {\n",
    "    'application': 'multiclass', # for binary classification\n",
    "#     'num_class' : 1, # used for multi-classes\n",
    "    'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 100, \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 62,\n",
    "    'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "    'max_depth': -1, # <0 means no limit\n",
    "    'max_bin': 510, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 5, # L1 regularization\n",
    "    'lambda_l2': 10, # L2 regularization\n",
    "    'metric' : 'multi_logloss',\n",
    "    'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}\n",
    "\n",
    "# Initiate classifier to use\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          objective = 'multiclass', \n",
    "          n_jobs = 5, \n",
    "          num_class = 3,\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'])\n",
    "\n",
    "# To view the default model parameters:\n",
    "mdl.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 4 folds for each of 3456 candidates, totalling 13824 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=-1)]: Done 8426 tasks      | elapsed: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9776 tasks      | elapsed: 34.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11226 tasks      | elapsed: 39.7min\n",
      "[Parallel(n_jobs=-1)]: Done 12776 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13824 out of 13824 | elapsed: 49.9min finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=LGBMClassifier(max_bin=510, min_child_samples=5,\n",
       "                                      min_child_weight=1, min_split_gain=0.5,\n",
       "                                      n_jobs=5, num_class=3,\n",
       "                                      objective='multiclass', subsample=1,\n",
       "                                      subsample_for_bin=200),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'boosting_type': ['gbdt', 'dart'],\n",
       "                         'colsample_bytree': [0.64, 0.65, 0.66],\n",
       "                         'learning_rate': [0.005, 0.01], 'max_bin': [255, 510],\n",
       "                         'n_estimators': [8, 16, 24],\n",
       "                         'num_leaves': [6, 8, 12, 16],\n",
       "                         'objective': ['multiclass'], 'random_state': [500],\n",
       "                         'reg_alpha': [1, 1.2], 'reg_lambda': [1, 1.2, 1.4],\n",
       "                         'subsample': [0.7, 0.75]},\n",
       "             verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'boosting_type': 'dart', 'colsample_bytree': 0.64, 'learning_rate': 0.01, 'max_bin': 255, 'n_estimators': 16, 'num_leaves': 16, 'objective': 'multiclass', 'random_state': 500, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.7}\n0.5884965473948525\n"
     ]
    }
   ],
   "source": [
    "gridParams = {\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'n_estimators': [8,16,24],\n",
    "    'num_leaves': [6,8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
    "    'objective' : ['multiclass'],\n",
    "    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
    "    'random_state' : [500],\n",
    "    'colsample_bytree' : [0.64, 0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=1, cv=4, n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "f:\\JetBrain Project Files\\Pycharm\\datastorm-2021\\venv\\lib\\site-packages\\lightgbm\\engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "[LightGBM] [Warning] Using too small ``bin_construct_sample_cnt`` may encounter unexpected errors and poor accuracy.\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=1 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=10, reg_lambda=1 will be ignored. Current value: lambda_l2=10\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 360\n",
      "[LightGBM] [Info] Number of data points in the train set: 57348, number of used features: 15\n",
      "[LightGBM] [Warning] lambda_l1 is set=5, reg_alpha=1 will be ignored. Current value: lambda_l1=5\n",
      "[LightGBM] [Warning] lambda_l2 is set=10, reg_lambda=1 will be ignored. Current value: lambda_l2=10\n",
      "[LightGBM] [Info] Start training from score -1.094540\n",
      "[LightGBM] [Info] Start training from score -1.100235\n",
      "[LightGBM] [Info] Start training from score -1.101074\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[4]\ttraining's multi_logloss: 1.08193\tvalid_1's multi_logloss: 1.08234\n",
      "[8]\ttraining's multi_logloss: 1.06639\tvalid_1's multi_logloss: 1.06712\n",
      "[12]\ttraining's multi_logloss: 1.0519\tvalid_1's multi_logloss: 1.0529\n",
      "[16]\ttraining's multi_logloss: 1.03789\tvalid_1's multi_logloss: 1.03921\n",
      "[20]\ttraining's multi_logloss: 1.02495\tvalid_1's multi_logloss: 1.02661\n",
      "[24]\ttraining's multi_logloss: 1.01254\tvalid_1's multi_logloss: 1.01449\n",
      "[28]\ttraining's multi_logloss: 1.00046\tvalid_1's multi_logloss: 1.00264\n",
      "[32]\ttraining's multi_logloss: 0.988965\tvalid_1's multi_logloss: 0.991358\n",
      "[36]\ttraining's multi_logloss: 0.978486\tvalid_1's multi_logloss: 0.98116\n",
      "[40]\ttraining's multi_logloss: 0.967884\tvalid_1's multi_logloss: 0.970834\n",
      "[44]\ttraining's multi_logloss: 0.958085\tvalid_1's multi_logloss: 0.961302\n",
      "[48]\ttraining's multi_logloss: 0.948573\tvalid_1's multi_logloss: 0.952\n",
      "[52]\ttraining's multi_logloss: 0.939911\tvalid_1's multi_logloss: 0.943631\n",
      "[56]\ttraining's multi_logloss: 0.931266\tvalid_1's multi_logloss: 0.935193\n",
      "[60]\ttraining's multi_logloss: 0.923544\tvalid_1's multi_logloss: 0.927648\n",
      "[64]\ttraining's multi_logloss: 0.916059\tvalid_1's multi_logloss: 0.920376\n",
      "[68]\ttraining's multi_logloss: 0.908265\tvalid_1's multi_logloss: 0.912678\n",
      "[72]\ttraining's multi_logloss: 0.901352\tvalid_1's multi_logloss: 0.905993\n",
      "[76]\ttraining's multi_logloss: 0.89434\tvalid_1's multi_logloss: 0.899175\n",
      "[80]\ttraining's multi_logloss: 0.887895\tvalid_1's multi_logloss: 0.892884\n",
      "[84]\ttraining's multi_logloss: 0.881793\tvalid_1's multi_logloss: 0.886946\n",
      "[88]\ttraining's multi_logloss: 0.876157\tvalid_1's multi_logloss: 0.881481\n",
      "[92]\ttraining's multi_logloss: 0.870708\tvalid_1's multi_logloss: 0.876205\n",
      "[96]\ttraining's multi_logloss: 0.865215\tvalid_1's multi_logloss: 0.870903\n",
      "[100]\ttraining's multi_logloss: 0.860094\tvalid_1's multi_logloss: 0.865943\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's multi_logloss: 0.860094\tvalid_1's multi_logloss: 0.865943\n"
     ]
    }
   ],
   "source": [
    "params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
    "params['learning_rate'] = grid.best_params_['learning_rate'] \n",
    "params['num_class'] = 3\n",
    "params['max_bin'] = grid.best_params_['max_bin']\n",
    "params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
    "params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
    "params['subsample'] = grid.best_params_['subsample']\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_tr, y_tr, test_size=0.1, random_state = 12)\n",
    "    \n",
    "#del X, y; gc.collect();\n",
    "\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "d_valid = lgb.Dataset(X_valid, label=y_valid) \n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, early_stopping_rounds=50, verbose_eval=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_df.iloc[:, :-1]\n",
    "y_val = val_df.iloc[:, -1]-1\n",
    "pred_y = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x.argmax()\n",
    "\n",
    "y_pred = np.apply_along_axis(func,1,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2462338456832607\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_val, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}